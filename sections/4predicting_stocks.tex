\chapter{Predicting Stocks}\label{ch:predictions}


\section{Predictions Using Machine Learning}

\section{Predictions Using Time Series}
\subsection{Idea/Process and Evaluation}
irgendwas in der Richtung: wir benutzen Time Series Modelle, machen Predictions und gucken uns am Ende dann den Mean Squared Error an. Sinnvollerweise immer die ersten 10 Perioden verwerfen, um den MSE vergleichbar zu machen zwischen allen Gruppen, auch denen, bei denen die ersten paar Perioden nicht definiert sind. 	
In-Sample vs. Out of Sample Prediction?


\subsection{Theoretical Overview}
Time series predictions in this paper will be made using Random Walks, autoregressive (AR), moving average (MA) and generalized autoregressive conditional heteroscedasticity (GARCH) models. The following will first give a short theoretical overview. Then different models will be applied to a training data set of two chosen stocks. Then predictions will be made for the other stocks using the above mentioned techniques. 

\subsubsection{Random Walks}
Random walks serve as the baseline against which every prediction can be compared. Assuming a random walk as the underlying process implies that we know nothing about the future and can do no better than assuming tomorrow's stock price will on average be the same as today. Formally, a random walk follows \begin{align}
    y_t &= y_{t-1} + w_t \label{eg:rw1}\\
    \intertext{where $y_t$ is the value of the time series at time t and $w_t$ is a random realisation of a stationary white noise process with mean 0 and variance $\sigma^2$. We can expand equation \ref{eg:rw1} by allowing for a constant trend, a drift. A random walk with drift can be represented as}
    y_t &= \delta + y_{t-1} + w_t
\end{align}{}
where $\delta$ is a drift parameter. Predictions for period t + 1 are therefore exactly the value at time t. As we have already eliminated the trend by transforming the data to log-returns we will not use the drift representation here. If we did our analysis on the original stock values then a drift would be appropriate. Note that the random walk (with or without drift) is not a stationary process. 

\subsubsection{Autoregressive Models}
An autoregressive process of order p (AR(p)) implies that the current value of a time series can be described as a combination of the previous p values plus a random shock. As those previous values intern depend on previous values, the current value is indirectly influenced by its entire past. Formally, an AR(p) process follows 
\begin{align}
    y_t &= \psi_1 y_{t-1} + \psi_2 y_{t-2} + ... + \psi_p y_{t-p} + w_t \label{eq:AR(p)}
\intertext{where $y_t$ is stationary, $\psi_1, ..., \psi_p$ are constants and $w_t$ is white noise. The mean of $y_t$ is assumed to be zero. If the mean is $\mu$ instead of zero, equation \ref{eq:AR(p)} can be rewritten as}
    y_t - \mu &= \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2} - \mu) + ... + \phi_p (y_{t-p} - \mu) + w_t \label{eq:AR(p)} \\
\intertext{This can also be expressed as}
    y_t &= \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + w_t
\end{align}
\noindent with $\alpha$ = $\mu (1 - \phi_1 - ... - \phi_p)$.

\subsubsection{Moving Average Models}
A moving average process of order q implies that the current value of a time series consists of the average of the previous q observations plus a random shock. As the mean of the time series $\mu$ is constant this average can also be simply expressed as an average of the past random shocks $\{w_{t-1}, ... w_{t-q}\} $. In constrast to the AR(p) process, the shocks affect the future directly (and not only indirectly through past values) and only affect the next q values. Formally, the MA(q) process can be expressed as
\begin{align}
    y_t = \mu + w_t + \theta w_{t-1} + ... + \theta w_{t-q}
\end{align}{}
\noindent where $w_t$ represents white noise and $\theta_1, ..., \theta_q$ are parameters and q is the number of lags in the moving average. 


\subsubsection{Autoregressive Moving Average Models}
Autoregressive Moving Average Models of order p and q (ARMA(p,q)) form a combination of the above described AR(p) and MA(q) models. Formally, an ARMA(p,q) process follows
\begin{align}
    y_t = \phi_1 y_{t-1} + ... + \phi_p y_{t-p} + w_t + \theta_1 w_{t-1} + ... + \theta_q w_{t-q} \\
    \intertext{if the mean of $y_t$ is $\mu$, then the above results in}
    y_t = \alpha + \phi_1 y_{t-1} + ... + \phi_p y_{t-p} + w_t + \theta_1 w_{t-1} + ... + \theta_q w_{t-q}
\end{align}
\noindent with $\alpha = \mu (1 - \phi_1 - ... - \phi_p)$.


\subsubsection{GARCH Models}
The GARCH(p,q) model is specified as follows:
\begin{align}
    r_{t} &= \sigma_t  \upepsilon_t \label{eq:garch1}\\
\intertext{where $\upepsilon_t$ is Gaussian white noise with  $\upepsilon_t \sim \mathcal{N}(0,1)$ and}
    \sigma^2_t &= \alpha_0 + \underbrace{\alpha_1 r^2_{t-1} + ... + \alpha_p r^2_{t-p}}_\text{autoregressive part} + \underbrace{\beta_1 \sigma^2_{t-1} + ... + \beta_q \sigma^2_{t-q}}_\text{moving average part} \label{eq:garch2}
\end{align}{}
In equation \ref{eq:garch1} the returns $r_t$ are therefore modelled as white noise with mean zero and variance variance $\sigma^2_t$. When compared to white Gaussian noise with constant variance this can produce a leptokurtic (fat-tailed) distribution similar to what we observed in the QQ-Plots in figure XXXXX. Equation \ref{eq:garch1} is called the mean model of the GARCH(p,q) process. This mean model can also be altered as needed. The GARCH model can then be specified in the following way: 
\begin{equation}
    r_t = x_t + y_t
\end{equation}{}
where $x_t$ can be any constant mean, regression or time series process and $y_t$ is a GARCH process that satisfies equations \ref{eq:garch1} and \ref{eq:garch2}. In a similar way, the distribution of $\upepsilon_t$ can be altered. In praxis, researchers often assume a t-distribution instead of a standard normal distribution. 

A further expansion is the GJR-GARCH model (SOURCE) where the variance GJR-GARCH(1,1,1) is specified as follows: 

\begin{equation}
    \sigma^2_t = \omega + \alpha * \epsilon^2_{t-1} + \gamma \epsilon^2_{t-1} I(\epsilon^2_{t-1} < 0) + \beta \sigma^2_{t-1} 
\end{equation}{}

This means that negative shocks may have a different impact on future volatility than positive shocks, e.g. a sudden drop in a stock will cause the stock to be disproportionally more volatile in the near future. 



\subsection{Approaching the Training Data}
\subsubsection{Data Exploration}
To get a better feeling about our data and to avoid overfitting we try to explore the two stocks INTC and V. We apply different time series models to the entire time series and check their model fit. Figure \ref{fig:INTC_V_ACF_log_returns} shows the ACF and PACF for the log-returns of INTC and V. From looking at the plots we can presume that for V, AR and MA models of order one or two might be a reasonable try. For INTC it looks like there is very little information included as none of the lower order lags bear any significance. 

We start by applying a formal test for autocorrelation to the time series, the Box/Pierce and Ljung/Box tests. They have slightly different properties regarding their handling of very large and very small numbers of observations, but for both the null hypothesis is that there is no autocorrelation in the series. Figure \ref{fig:ljungbox} shows the p-values for the first 40 lags. The test suggests that for V there may be some significant autocorrelations that could be used for modelling. Compared to the plot of ACF and PACF the test even seems too optimistic. For INTC, the test confirms non-significance for the first few lags. While higher order lags are deemed to be significant by the test modelling a time series process with so many coefficients would be very prone to overfitting. 

\begin{figure}[h]
    \centering
    \figuretitle{ACF and PACF of log-returns of Stocks INTC and V}
    \begin{adjustbox}{width=.95\textwidth,center}
    \input{figures/INTC_ACF_log_returns.pgf}
    \end{adjustbox}
    
    \begin{adjustbox}{width=.95\textwidth,center}
    \input{figures/V_ACF_log_returns.pgf}
    \end{adjustbox} 
    
    \caption{}
    \label{fig:INTC_V_ACF_log_returns}
\end{figure}{}

\begin{figure}[h]
    \centering
    \figuretitle{Ljung-Box and Box-Pierce Test for Autocorrelation}
    \begin{adjustbox}{width=.95\textwidth,center}
    \input{figures/V_ljung_box.pgf}
    \input{figures/INTC_ljung_box.pgf}
    \end{adjustbox}
    \caption{}
    \label{fig:ljungbox}
\end{figure}{}

\subsubsection{ARMA models}
We start by fitting ARMA models to the time series. This turned out to be prone to numerical instability. While the BIC could always be calculated, for some of the models standard errors could not be computed as the software was not able to invert the Hessian matrix. The underlying problem is exacerbated when dealing with GARCH models, as all residuals are squared. The problem was eventually alleviated by multiplying our log-returns by 100 (corresponding to an approximate percentage interpretation). Table \ref{tab:bic_arma} shows the BIC that were obtained from fitting different ARMA(p,q) models to the series of log-returns. 

\begin{table}
    \centering
    \input{figures/table_bic_arma.tex}
    \caption{BIC presented for different combinations of ARMA(p,q) fit to the log-returns of V (top) and INTC (bottom). On the right side, those returns were multiplied by 100 in order to allow for comparison with the GARCH models later on.}
    \label{tab:bic_arma}
\end{table}{}

For V the best model according to BIC is ARMA(1,1). However the difference to ARMA(0,0) seems as good as negligible. In order to avoid the peril of reading too much into random chance, it may be more prudent to stick with a constant mean model (ARMA(0,0)). For INTC the lowest BIC is indeed reached by ARMA(0,0) which corresponds to our observation that there is no significant lower-level autocorrelation. 

Tables \ref{tab:V_ARMA11_log_returns}, \ref{} show the results of fitting an  ARMA(1,1) and an ARMA(0,0) model to the log-returns of V. The change in AIC is larger than the change in BIC as the latter more heavily penalizes the number of parameters included in the estimation. Interestingly, even though the difference in BIC is quite small, the AR(1) and MA(1) are estimated very significantly and are high in relative magnitude. However, note that their effects go in opposing directions in similar magnitude and may as well cancel each other out. This interpretation may be somewhat plausible as the individual effects of the AR(1) and MA(1) terms are about an order of magnitude smaller (albeit still significant) and go in the same direction when estimating ARMA(1,0) and ARMA(0,1) models separately (-0.0736 for the AR(1) and -0.0809 for the MA(1) term).
Table \ref{tab:INTC_ARMA00_log_returns} shows the result of fitting an ARMA(0,0) model to the log-returns of INTC. Not even the mean is significant, which corresponds to the development of the stock prices of INTC over the time period. As could be seen back in figure \ref{fig:Daily Stock Prices for all Stocks in the Data Set}, Intel has hardly gained in value from 2012 to 2017. Also when exploratively trying to fit an ARMA(1,0), ARMA(0,1) or ARMA(1,1) model none of the coefficients reach significance (p-values all > 0.5). 

\begin{table}[h]
    \centering
    \figuretitle{Results for an ARMA(1,1) process fit to the log-returns of V}
    \input{figures/V_result_ARMA11.tex}
    \caption{}
    \label{tab:V_ARMA11_log_returns}
\end{table}{}

\begin{table}[h!]
    \centering
    \figuretitle{Results for an ARMA(0,0) process fit to the log-returns of V}
    \input{figures/V_result_ARMA00.tex}
    \caption{}
    \label{tab:V_ARMA11_log_returns}
\end{table}{}

\begin{table}[h!]
    \centering
    \figuretitle{Results for an ARMA(0,0) process fit to the log-returns of INTC}
    \input{figures/INTC_result_ARMA00.tex}
    \caption{}
    \label{tab:INTC_ARMA00_log_returns}
\end{table}{}


--> make predictions
--> calculate MSE
--> make test for trivial forecasts

\subsubsection{ARMAX models}
We proceed in our analysis by adding our sentiments generated by the machine learning model and the ravenpack sentiments as external regressors. To make matters more complicated our stock market observations don't match the external data. We have about a third as many analyst reports (and therefore sentiment scores) as stock market observations. In order to obtain an estimable model we decided to discard all observations on days where no analyst reports existed. This also implied that we had to confine our analysis to a constant mean model (ARMA(0,0) as autoregressive and moving-average components do not make sense when many observations in the time series are missing. As explained previously, this should not be problematic. The model hence reduces to a regression with an intercept and the sentiment data as independent variables. The Ravenpack data, on the other side, usually had multiple entries per day with only few days missing. We therefore aggregated sentiments to obtain one single observation per day. 
%To allow for better comparison, we also confined ourselves to 

\textit{ARMAX Sentiments Analyst Reports}
The fact that we had to omit two-thirds of all data points changes the BIC considerably. To be able to compare the models according to BIC we refit the ARMA(0,0) model to the reduced data and obtained a BIC of -3001.07 for V and -4533.63 for INTC (the difference owing to the different number of observations). Table \ref{tab:V_result_ARMAX00_sentiment} shows the results for V, table \ref{tab:INTC_result_ARMAX00_sentiment} shows the results for INTC. Even though sent\_mean is not too far from significance for V (and is even closer when only including sent\_mean, with a p-value of 0.067), all models fare worse in terms of BIC than the baseline. 

\begin{table}[h]
    \centering
    \figuretitle{ARMAX(0,0) with analyst report sentiments fit to the log-returns of V}
    \input{figures/V_result_ARMAX00_sentiment.tex}
    \caption{}
    \label{tab:V_result_ARMAX00_sentiment}
\end{table}{}

\begin{table}[h!]
    \centering
    \figuretitle{ARMAX(0,0) with analyst report sentiments fit to the log-returns of INTC}
    \input{figures/INTC_result_ARMAX00_sentiment.tex}
    \caption{}
    \label{tab:INTC_result_ARMAX00_sentiment}
\end{table}{}

\textit{ARMAX Sentiments Ravenpack}
In the Ravenpack data for V, only 21 observations out of 1509 were missing. We decided it might still be interesting to continue having a look at ARMA(1,1) even though the estimation is now mildly distorted by the fact that some values are missing. We computed a new baseline BIC as we did in the previous analysis. The BIC for an ARMA(0,0) model is now -8734.35 and for an ARMA(1,1) model -8735.23. For INTC there were observations for every day of the time series, so the baseline BIC stayed -8692.98. Table \ref{tab:V_result_ARMAX00_ravenpack} shows the result for the ARMAX(0,0) model fit with ravenpack sentiments to V, table \ref{tab:INTC_result_ARMAX00_ravenpack}. We have tried different combinations of regressors to include in the model but only show the full model to avoid redundancy. Also the results for ARMAX(1,1) for V are not shown but look very similar. In all cases the external information worsened BIC and did not improve the fit. For the following GARCH analysis we will therefore not include external information. 

\begin{table}[h]
    \centering
    \figuretitle{ARMAX(0,0) with Ravenpack sentiments fit to the log-returns of V}
    \input{figures/V_result_ARMAX00_raven.tex}
    \caption{}
    \label{tab:V_result_ARMAX00_ravenpack}
\end{table}{}

\begin{table}[h]
    \centering
    \figuretitle{ARMAX(1,1) with Ravenpack sentiments fit to the log-returns of V}
    \input{figures/V_result_ARMAX11_raven.tex}
    \caption{}
    \label{tab:V_result_ARMAX00_sentiment}
\end{table}{}

\begin{table}[h]
    \centering
    \figuretitle{ARMAX(0,0) with Ravenpack sentiments fit to the log-returns of INTC}
    \input{figures/INTC_result_ARMAX00_raven.tex}
    \caption{}
    \label{tab:INTC_result_ARMAX00_ravenpack}
\end{table}{}

\subsubsection{GARCH models}
Financial data often exhibit conditional heteroskedasticity. We therefore want to see whether GARCH models are able to improve the fit. Figure \ref{fig:V_INTC_squared} shows the squared log-returns of V and INTC as well as their ACF and PACF. For both the first lag visually seems to be significant. 

\begin{figure}[h]
    \centering
    \figuretitle{Squared log-returns V and INTC}
    \begin{adjustbox}{width=.95\textwidth,center}
    \input{figures/V_squared_log_returns.pgf}
    \input{figures/INTC_squared_log_returns.pgf}
    \end{adjustbox}
    \hspace{3ex}
    \figuretitle{ACF and PACF of Squared log-returns V and INTC}
    \begin{adjustbox}{width=.95\textwidth,center}
    \input{figures/V_ACF_squared_log_returns.pgf}
    \end{adjustbox}
    \begin{adjustbox}{width=.95\textwidth,center}
    \input{figures/INTC_ACF_squared_log_returns.pgf}
    \end{adjustbox}
    \caption{Squared log-returns and ACF and PACF for squared log-returns for V (top-left and middle) and INTC (top right and bottom). }
    \label{fig:V_INTC_squared}
\end{figure}{}

We formally test for the presence of a GARCH effect, i.e. autocorrelation in the squared residuals of the time series using the Lagrange Multiplier Test proposed by Engle (SOURCE). For both tests, the null hypothesis of 'No ARCH effect' cannot be rejected with p-values of 0.1160 (V) and 0.2693 (INTC). We nevertheless proceed to fit a GARCH model to the data. There is extensive literature on the optimal order for a GARCH model (e.g. does anything beat GARCH(1,1), does anything not beat GARCH(1,1), does anyone need GARCH(1,1)?). 

For simplicity, we will stick to a GARCH(1,1) model with a constant mean model (the mean is modeled by an ARMA(0,0) process). While we have tried other models such as for example GARCH(3,1) they did not perform significantly better than GARCH(1,1). To avoid issues with numerical instability all log-returns are multiplied by 100 and the new current best BIC become 5025.46 for V and 5205.42 for INTC as shown on the right hand side of \ref{tab:bic_arma}. Table \ref{tab:V_result_GARCH11_100} shows the result for a GARCH(1,1) fit to V, table \ref{tab:INTC_result_GARCH11_100} shows the result for INTC. In both cases the fit improves and the BIC drops - not very much, but noticeably (from 5025.46 to 4948.68 (V) and from 5205.42 to 5169.75 (INTC)). 

\begin{table}[h]
    \centering
    \figuretitle{Results for GARCH(1,1) with constant mean fit to the log-returns of V}
    \input{figures/V_result_GARCH11_100.tex}
    \caption{}
    \label{tab:V_result_GARCH11_100}
\end{table}{}

\begin{table}[h!]
    \centering
    \figuretitle{Results for GARCH(1,1) with constant mean fit to the log-returns of V}
    \input{figures/INTC_result_GARCH11_100.tex}
    \caption{}
    \label{tab:INTC_result_GARCH11_100}
\end{table}{}

We can relax the assumption of normally distributed error terms $\upepsilon_t$ (see \ref{eq:garch1}) and instead assume a Student t-distribution with fatter tails. This time the BIC drops considerably (from 4948.68 to 4794.91 (V) and from 5169.75 to 4986.41 (INTC)). The results are shown in table \ref{tab:V_result_GARCH11_students_100} and table \ref{tab:INTC_result_GARCH11_students_100}. 

\begin{table}[h]
    \centering
    \figuretitle{Results for GARCH(1,1) with t-Distribution (V)}
    \input{figures/V_result_GARCH11_100_students.tex}
    \caption{}
    \label{tab:V_result_GARCH11_students_100}
\end{table}{}

\begin{table}[h!]
    \centering
    \figuretitle{Results for GARCH(1,1) with t-Distribution (INTC)}
    \input{figures/INTC_result_GARCH11_100_students.tex}
    \caption{}
    \label{tab:INTC_result_GARCH11_students_100}
\end{table}{}

Another expansion is the inclusion of asymmetric shocks (see \ref{eq:garch3}). This implies that sudden drops in a stock's value lead to higher volatility than an increase of the same amount would. Results for the GJR-GARCH model are shown in tables \ref{tab:V_result_GARCH11_students_GJR_100} and \ref{tab:INTC_result_GARCH11_students_GJR_100}. For V, the gamma-coefficient is positive and significant, indicating an asymmetric effect of shocks on volatility. BIC drops by a bit from 4794.91 to 4784.75. For INTC, the gamma coefficient is not significant and BIC even slightly increase from 4986.41 to 4993.23.

\begin{table}[h]
    \centering
    \figuretitle{Results for GJR-GARCH(1,1) with t-Distribution (V)}
    \input{figures/V_result_GARCH11_100_students_GJR.tex}
    \caption{}
    \label{tab:V_result_GARCH11_students_GJR_100}
\end{table}{}

\begin{table}[h!]
    \centering
    \figuretitle{Results for GJR-GARCH(1,1) with t-Distribution (INTC)}
    \input{figures/INTC_result_GARCH11_100_students_GJR.tex}
    \caption{}
    \label{tab:INTC_result_GARCH11_students_GJR_100}
\end{table}{}

\subsubsection{Forecasting and Forecast Precision}
To assess the predictive performance of we use the ARMA(0,0) model vor V and INTC as well as the ARMA(1,1) model for V to generate predictions. To do so, we fit an ARMA model to the time series up to time $y_t$, predict the next value $\hat{y}_{t+1}$, then add the true value $y_{t+1}$ to the time series and fit a model that lets us predict $\hat{y}_{t + 2}$ and so forth. For the ARMA(0,0) model, this means predicting the next value on the basis of a cumulative mean of the past log-returns. Estimating an ARMA(1,1) consecutively for V turned out to be problematic because the result was not always a stationary process. Therefore, at some points the estimation of an ARMA(1,1) model was impossible. We therefore had to replace the prediction for those specific points with ARMA(0,0). 
To compare the results we compared the mean squares error (MSE) as well as a binary prediction accuracy that merely judges whether the direction of the prediction was correct. The predictions of the ARMA(0,0) model with constant mean (which corresponds to a random walk for the stock prices with drift) seem to be better then mere coin tosses. However they do not perform better than the naive strategy of always predicting an upwards movement of the stock (i.e. a positive return). Only the ARMA(1,1) model for V reaches a higher prediction accuracy. This might, however, also be due to chance. Table \ref{tab:V_INTC_ARMA_predictions} shows the results of those predictions. The forecasts are shown in figure \ref{fig:V_INTC_ARMA_predictions_plot}. 

\begin{table}[]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    {}  & MSE & Binary Accuracy & Naive Binary Accuracy \\
    \midrule
    V - ARMA(0,0) & 0.2457 & 54.1528 \% & \textbf{54.2193} \% \\
    V - ARMA(1,1) & 0.2473 & \textbf{55.8139} & 54.2193 \\
    INTC - ARMA(0,0) & 0.2760 & 51.7608 & \textbf{52.3588} \\
    \bottomrule
    \end{tabular}
    \caption{Caption}
    \label{tab:V_INTC_ARMA_predictions}
\end{table}{}

\begin{figure}
    \centering
    %\includegraphics{}
    \caption{Caption}
    \label{fig:V_INTC_ARMA_predictions_plot}
\end{figure}{}


\subsubsection{Forecasting Volatility}
While GARCH models are not able to better predict future returns, they can help predict the volatility of future returns. Figure \ref{fig:} gives an intuition to the usefulness of volatility forecasts. In the figure, the volatility of V predicted by an GJR-GARCH(1,1) process is plotted together with the absolute value of the log-returns of the stock. We can clearly see that periods that visually look more volatile often coincide with a high predicted volatility. 

This is useful for balancing portfolios, pricing options and XXX [THEORY!]. As the main focus of this paper is the prediction of returns, volatility forecasts will not be further discussed in detail. 




\subsection{Weighted Average of Predictions}
a) of different time series models
b) of time series and ML models






\section{IRGENDWO ANDERS EINFÜGEN}
The movement of time series data for financial data is influenced by external effects and information. To leverage this we tried to add information from news sources to our trading strategies. For the hybrid prediction we predict sentiment scores on news sources. The original aim was to use financial news data to predict stock price movement and volatility for trading strategies. To achieve this, large amounts of text data would need to be preprocessed and analyzed regarding their connections to specific stocks, their topic and sentiment. The news data would need to be as precise as possible, because […] mention that an effect on the stocks an only be measured up to 20min after the news appear. Other sources say that… .
%
As we were not able to acquire access to a reliable and precise news sources, we tried to implement our approach on the available analyst reports regarding specific stocks. The problem with these reports is, that they are more an indicator of performance over the past month and a prediction about the future performance and don´t cover sudden events. The reports also cluster around (meetings?) with long stretches of no or very few reports in between. This makes it unlikely that they are valuable for trading strategies.
%
The goal was to identify the connection of specific articles to listed companies and compute a sentiment score for the article. There are many ways to calculate sentiment scores, like positive and negative, from text data. Many of these require a supervised approach … . Language is very context specific (…) making it unpractical to use other, labelled training data sets, than financial news data(…). With the use of intra day trading and news data one could have also used the movement or volatility of the period close after the news release to get a rough estimate of the impact certain news have. Such an approach was chosen by \citet{robertson2007news}. Our data is only inter day and does not allow for a classification in that way. 

A common approach for unsupervised 


Analyst report data beschreiben...

To get reliable sentiment scores text data has to be preprocessed. The preprocessing was done using \texttt{R} \citep{Rproject}. At first words where converted to lowercase and tokenized using the R package \textit{tidytext} \citep{tidytext}. Next all the stop words where removed using the stop word library from the \textit{tidytext} package, as well as a custom set. In the next step all links to websites, hyper-references, numbers and words with numbers are removed as well. 
The last step is lemmatizing the words using the \textit{textstem} package \citep{textstem}. Lemmatizing words means reducing them to their inflectional forms. Commonly stemming is also applied, because words sometimes have derivationally related forms. This was not done to have more flexibility for the later applied text analysis. Additionally we could have also used the term frequency–inverse document frequency (tf-idf) matrix (ZITIEREN) for further reductions in the number of words. The issue here would have been that highly informative words for the stock sentiment could have been removed. 
